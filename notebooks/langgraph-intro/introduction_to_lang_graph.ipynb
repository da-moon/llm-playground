{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ðŸ¤ Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mInstalling dependencies from lock file\u001b[39m\n",
            "\n",
            "No dependencies to install or update\n"
          ]
        }
      ],
      "source": [
        "!poetry install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangGraph - Intro\"\n",
        "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_core.tools import BaseTool\n",
        "from typing import List\n",
        "tool_belt:List[BaseTool] = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "alternatively , you can use `load_tools` helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import BaseTool\n",
        "from typing import Sequence,Union,Callable\n",
        "from langchain.agents import load_tools \n",
        "# NOTE: load_tools returns `List[BaseTool]` while `ToolNode` expects\n",
        "# `Sequence[Union[BaseTool, Callable]]`\n",
        "tool_belt:Sequence[Union[BaseTool, Callable]] = load_tools([\n",
        "    \"arxiv\",\n",
        "    \"ddg-search\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "llm : BaseChatModel = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.language_models import LanguageModelInput\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "runnable : Runnable[LanguageModelInput, BaseMessage] = llm.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ”Answer #1:\n",
        "\n",
        "\n",
        "The model determines which tool to use through a few key mechanisms:\n",
        "\n",
        "1. Function calling capabilities: As mentioned in the OpenAI Developer Forum\n",
        "   discussion, models like GPT have been fine-tuned on using functions. The\n",
        "   model analyzes the user's input and determines if calling a particular\n",
        "   function/tool would be helpful in generating a relevant response [1] [2].\n",
        "\n",
        "2. Tool descriptions: The tools are described to the model, usually in the\n",
        "   system message or as part of the function specifications. The model uses\n",
        "   these descriptions to understand when each tool would be appropriate to use.\n",
        "   Essentially, in the first API call to the model, the user includes a\n",
        "   `description` field in objects under `functions` which tells the model what\n",
        "   each function does.\n",
        "   [1] [2].\n",
        "\n",
        "3. Perception of external tool utility: The model makes a judgment on whether an external tool can better satisfy the user's needs than its own knowledge alone. For example, it may decide to use a web search tool for very recent information or a code interpreter for mathematical calculations [1].\n",
        "\n",
        "4. Training on tool usage patterns: Models are often trained on examples of when different tools are typically used, allowing them to learn appropriate usage patterns [3].\n",
        "\n",
        "5. Prompt engineering: The way tools are presented to the model in the prompt can influence when and how it chooses to use them [4].\n",
        "\n",
        "\n",
        "\n",
        "[1]:\n",
        "      https://community.openai.com/t/how-does-the-assistants-api-select-tools/516221\n",
        "[2]: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n",
        "[3]: https://www.ibm.com/topics/ai-model\n",
        "[4]: https://viso.ai/deep-learning/ml-ai-models/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated,List\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "# NOTE: reference\n",
        "# https://github.com/langchain-ai/langgraph/blob/a93775413281df9ddf6ba29cc388b2460d94b9af/libs/langgraph/langgraph/graph/state.py#L80\n",
        "reducer = add_messages\n",
        "class AgentState(TypedDict):\n",
        "  # messages: Annotated[list, add_messages]\n",
        "  messages: Annotated[List[BaseMessage], reducer]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create our graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "uncompiled_graph:StateGraph = StateGraph(AgentState)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create and add our first node to the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages.base import BaseMessage\n",
        "from langchain_core.runnables import Runnable\n",
        "from typing import List\n",
        "\n",
        "def call_model(state:AgentState,runnable: Runnable ) -> dict[str, list[BaseMessage]]:\n",
        "  messages:List[BaseMessage] = state[\"messages\"]\n",
        "  response:BaseMessage = runnable.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "uncompiled_graph.add_node(\n",
        "    node = \"agent\",\n",
        "    action = lambda state: call_model(state, runnable),\n",
        "    metadata = {\"purpose\": \"call LLM\" }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create and add our second node to the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tool_node: ToolNode = ToolNode(\n",
        "  tools=tool_belt,\n",
        "  tags=[ \"arxiv\", \"ddg-search\"],\n",
        "  handle_tool_errors = True,\n",
        ")\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "def should_continue(state:StateGraph):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ”Answer #2:\n",
        "\n",
        "There is default Recursion limit is `25` [1] . You can increase the limit by setting\n",
        "the `recursion_limit` [2] config key when calling `invoke()` on the graph object.\n",
        "\n",
        "Otherwise, you can use the following methods\n",
        "\n",
        "\n",
        "1. Add a counter to the state:\n",
        "   We can modify the `AgentState` class to include a counter:\n",
        "\n",
        "   ```python\n",
        "   class AgentState(TypedDict):\n",
        "     messages: Annotated[list, add_messages]\n",
        "     cycle_count: int\n",
        "   ```\n",
        "\n",
        "   Then, increment this counter in the `tool_call_or_helpful` function and check it against a maximum value:\n",
        "\n",
        "   ```python\n",
        "   def should_continue(state):\n",
        "     # ... start of the function\n",
        "     if 'cycle_count' not in state:\n",
        "      state['cycle_count'] = 1\n",
        "     state['cycle_count'] += 1\n",
        "     if state['cycle_count'] > MAX_CYCLES:\n",
        "       return \"end\"\n",
        "     # ... rest of the function\n",
        "   ```\n",
        "\n",
        "2. Use the length of the messages list in `should_continue` function:\n",
        "   As shown in the existing code, we can use the length of the messages list to limit cycles:\n",
        "\n",
        "   ```python\n",
        "   def should_continue(state):\n",
        "     # ... start of the function\n",
        "     if len(state[\"messages\"]) > MAX_MESSAGES:\n",
        "       return \"END\"\n",
        "     # ... rest of the function\n",
        "   ```\n",
        "\n",
        "   This approach is already implemented in the `tool_call_or_helpful` function, effectively limiting the number of cycles to 10.\n",
        "\n",
        "3. Add a timeout:\n",
        "   We could implement a timeout mechanism that ends the cycle after a certain\n",
        "   amount of time has passed. we can add an `if` branch in `should_continue`\n",
        "   function\n",
        "\n",
        "   ```python\n",
        "   def should_continue(state):\n",
        "     # ... start of the function\n",
        "     if 'start_time' not in state:\n",
        "      state['start_time'] = time.time()\n",
        "     elif time.time() - state['start_time'] > MAX_EXECUTION_TIME:\n",
        "      return END\n",
        "     # ... rest of the function\n",
        "   ```\n",
        "\n",
        "By implementing any of these methods, we can ensure that the agent doesn't get stuck in an infinite loop and always terminates after a certain number of cycles or amount of time.\n",
        "\n",
        "[1]: https://github.com/langchain-ai/langchain/blob/ef329f681915ee696a5ddbe0cf9bf87a4406cdec/libs/core/langchain_core/runnables/config.py#L126\n",
        "[2]: https://github.com/langchain-ai/langgraph/blob/main/examples/recursion-limit.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"tool_calls\": [\n",
            "                {\n",
            "                    \"id\": \"call_XZtbOyvhJTUCSfMyisfY26oq\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\":\\\"current captain of the Winnipeg Jets 2023\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                }\n",
            "            ],\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 25,\n",
            "                \"prompt_tokens\": 156,\n",
            "                \"total_tokens\": 181\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"tool_calls\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-03e75c5c-222e-451f-8f98-f46cca2334a5-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"current captain of the Winnipeg Jets 2023\"\n",
            "                },\n",
            "                \"id\": \"call_XZtbOyvhJTUCSfMyisfY26oq\",\n",
            "                \"type\": \"tool_call\"\n",
            "            }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 156,\n",
            "            \"output_tokens\": 25,\n",
            "            \"total_tokens\": 181\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"Lowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... Adam Lowry was named captain of the Winnipeg Jets on Tuesday. ... Sep 20, 2023. Latest News. Inside look at Vegas Golden Knights Aug 30, 2024. Vegas Golden Knights fantasy projections for 2024-25 The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club's new captain on Tuesday morning. \\\"When I ... Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ...\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_XZtbOyvhJTUCSfMyisfY26oq\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"The current captain of the Winnipeg Jets is Adam Lowry. He was named the captain on September 12, 2023.\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 27,\n",
            "                \"prompt_tokens\": 443,\n",
            "                \"total_tokens\": 470\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"stop\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-1b03a344-942b-4f2f-aa8c-f34ab40037ad-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 443,\n",
            "            \"output_tokens\": 27,\n",
            "            \"total_tokens\": 470\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(\n",
        "    inputs,\n",
        "    config={\"recursion_limit\": 10},\n",
        "    stream_mode=\"updates\"\n",
        "):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(json.dumps(\n",
        "            values[\"messages\"],\n",
        "            indent=4,\n",
        "            default=lambda x: x.__dict__,\n",
        "            ensure_ascii=False\n",
        "        ))\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"tool_calls\": [\n",
            "                {\n",
            "                    \"id\": \"call_xDPHsqFo8MVkh9STsy2dWKbO\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"QLoRA\\\"}\",\n",
            "                        \"name\": \"arxiv\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_C9i1lLV3KbtUlJERduDZwzlD\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"QLoRA paper authors\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                }\n",
            "            ],\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 53,\n",
            "                \"prompt_tokens\": 173,\n",
            "                \"total_tokens\": 226\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"tool_calls\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-02817aa3-0ec9-46fc-82bc-2daade0ded3d-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [\n",
            "            {\n",
            "                \"name\": \"arxiv\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"QLoRA\"\n",
            "                },\n",
            "                \"id\": \"call_xDPHsqFo8MVkh9STsy2dWKbO\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"QLoRA paper authors\"\n",
            "                },\n",
            "                \"id\": \"call_C9i1lLV3KbtUlJERduDZwzlD\",\n",
            "                \"type\": \"tool_call\"\n",
            "            }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 173,\n",
            "            \"output_tokens\": 53,\n",
            "            \"total_tokens\": 226\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[\n",
            "    {\n",
            "        \"content\": \"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"arxiv\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_xDPHsqFo8MVkh9STsy2dWKbO\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"Authors: Anand Ramachandran. ... of the Transformer architecture in the paper \\\"Attention Is All Y ou Need\\\" by V ... - QLoRA-fine-tuned models may require special handling for deployment due to ... The authors of the QLoRA paper created an information-theoretically optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats. This data type builds on top of quantile quantization, which ensures that each quantization bin (parts of the data) has an equal number of ... The authors of the QLoRA paper used a blocksize of 64 for W for higher quantization precision and a blocksize of 256 for c2 to conserve memory. For parameter updates, only the gradient with ... AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, ... We present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. ... In this paper, we present a new multipacking-tree (MP-tree) representation for macro ... Official code from paper authors Submit ... While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring ...\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_C9i1lLV3KbtUlJERduDZwzlD\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"tool_calls\": [\n",
            "                {\n",
            "                    \"id\": \"call_u712iJALlhurVJ0vQxigJoE5\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Tim Dettmers latest Tweet\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_W8FQ1LyrM5YBn5zqoCYMZB6D\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Artidoro Pagnoni latest Tweet\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_7VIzrX3L8zOLj3k6VNeVvbk1\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Ari Holtzman latest Tweet\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_dzs9u13QzVxM7te2RlljfSF6\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Luke Zettlemoyer latest Tweet\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                }\n",
            "            ],\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 108,\n",
            "                \"prompt_tokens\": 1523,\n",
            "                \"total_tokens\": 1631\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_fde2829a40\",\n",
            "            \"finish_reason\": \"tool_calls\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-2eb05c44-96ee-49f8-9129-bf6a479b2125-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Tim Dettmers latest Tweet\"\n",
            "                },\n",
            "                \"id\": \"call_u712iJALlhurVJ0vQxigJoE5\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Artidoro Pagnoni latest Tweet\"\n",
            "                },\n",
            "                \"id\": \"call_W8FQ1LyrM5YBn5zqoCYMZB6D\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Ari Holtzman latest Tweet\"\n",
            "                },\n",
            "                \"id\": \"call_7VIzrX3L8zOLj3k6VNeVvbk1\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Luke Zettlemoyer latest Tweet\"\n",
            "                },\n",
            "                \"id\": \"call_dzs9u13QzVxM7te2RlljfSF6\",\n",
            "                \"type\": \"tool_call\"\n",
            "            }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 1523,\n",
            "            \"output_tokens\": 108,\n",
            "            \"total_tokens\": 1631\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[\n",
            "    {\n",
            "        \"content\": \"Leading AI researcher Tim Dettmers estimates each ChatGPT query needs ~460,000 MacBook Pro-days worth of computing to generate responses. That's $100,000+ of laptop hardware per chat! Dettmers calculates ChatGPT likely uses $100,000 of cloud computing resources every hour across its globally distributed infrastructure. Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required â€” from over 780GB to less than 48GB â€” to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... In September 2024, TimTheTatman surprised fans by announcing his return to Twitch via a tweet, simply stating, \\\"I'm back btw,\\\" accompanied by a link to his Twitch stream. This return is especially significant because Tim has embraced a new strategy: multi-streaming. Magistrate Judge Thomas S. Hixson's protective order said the group of authors suing Meta for copyright infringement over its AI model cannot use the chat logs \\\"and must return them to Meta or destroy them.\\\". The authors, who include comedian Sarah Silverman and Pulitzer Prize-winner Andrew Sean Greer, argued that Meta waived its ... Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, ... (SpQR), a new compressed format and quantization technique that enables for the first time \\\\emph{near-lossless} compression of LLMs across model scales while reaching similar compression levels to previous methods. SpQR works by identifying and isolating \\\\emph{outlier ...\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_u712iJALlhurVJ0vQxigJoE5\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"Artidoro Pagnoni. artidoro. Follow. mrm8488's profile picture Weyaxi's profile picture nezubn's profile picture. ... artidoro/model-tvergho. Updated Nov 18, 2023. artidoro/model-vinaic. Updated Nov 18, 2023. artidoro/model-vinaia. Updated Nov 18, 2023. datasets. None public yet. Company Fine-tuning large language models (LLMs) is a common practice to adapt them for specific tasks, but it can be computationally expensive. LoRA (Low-Rank Adaptation) is a technique that makes this process more efficient by introducing small adapter modules to the model. These adapters capture task-specific knowledge without modifying the original ... efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages ... We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ...\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_W8FQ1LyrM5YBn5zqoCYMZB6D\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"The heated on-air dispute last night between MSNBC's Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn't end with Wednesday's segment: Today, Lewandowski tweeted a video in ... Ari Melber warns Trump campaign advisor Corey Lewandowski he 'will be potentially in a defamation situation' on Wednesday's episode of The Beat (MSNBC) \\\"I did not say that. That is a false ... Humans rate the disinformation generated by Grover as trustworthy, even more so than human-written disinformation. Thus, developing robust verification techniques against generators such as Grover is an important research area. We consider a setting in which a discriminator has access to 5000 Grover generations, but unlimited access to real news. . In this setting, the best existing fake news ... While in Mumbai, Ari would stay at the Holtzman's home each evening. \\\"Their house was like a second home to me,\\\" explains Ari, who has travelled to India forty times. The last time that Ari was at the Holtzman's Chabad House was two months ago, where he stayed for a week and participated in prayers and Torah lectures given by Rabbi ... Aria Holtzman '27 surprised her grandfather, Shaojing Zhang, the conductor of the Shenyang Philharmonic, by walking on stage while he was filming a TV show in China celebrating the Lunar New Year. Holtzman said her grandfather, who she hadn't seen in person for five years, paused for a moment when he was told they had a surprise for him.\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_7VIzrX3L8zOLj3k6VNeVvbk1\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"Luke Zettlemoyer, a professor at University of Washington and a research manager at Meta, discusses multimodal generative AI, visual grounding and embodiment in text-based models, advantages of discretization tokenization in image generation, self-alignment with instruction backtranslation, generalizability of language models, model performance and evaluation, the importance of open source and ... Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open source and open science. We explore the grounding problem, the need for visual grounding and embodiment in Jim \\\"Mattress Mack\\\" McIngvale, a well-known Houston businessman, has been hospitalized at St. Luke's due to massive leg swelling, a red rash, and memory loss. Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. Christian is a two-time All-Pro running back who's on the cover of the latest Madden video game, the reigning AP Offensive Player of the Year and perennially among the top fantasy football picks ...\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_dzs9u13QzVxM7te2RlljfSF6\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"Here are the latest updates and tweets from the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - Tim Dettmers is an AI researcher who has recently been involved in discussions about the computational requirements of ChatGPT. He estimates that each ChatGPT query needs a significant amount of computing power, equivalent to $100,000+ of laptop hardware per chat. He has also been recognized for his work on QLoRA, which significantly reduces the GPU memory required for finetuning large models.\\n\\n2. **Artidoro Pagnoni**:\\n   - Artidoro Pagnoni has been active in the field of efficient finetuning of large language models (LLMs). His recent work includes updates on models and datasets related to QLoRA. He has also contributed to understanding factuality in abstractive summarization.\\n\\n3. **Ari Holtzman**:\\n   - Ari Holtzman has been involved in discussions about disinformation and the robustness of verification techniques against generators like Grover. He has also shared personal experiences, such as his visits to India and family events.\\n\\n4. **Luke Zettlemoyer**:\\n   - Luke Zettlemoyer is a professor at the University of Washington and a research manager at Meta. He has been discussing topics related to multimodal generative AI, visual grounding, and the importance of open source and open science. He has also been involved in conversations about the generalizability of language models and their performance evaluation.\\n\\nThese updates provide a glimpse into the recent activities and contributions of the authors in the field of AI and machine learning.\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 328,\n",
            "                \"prompt_tokens\": 3018,\n",
            "                \"total_tokens\": 3346\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"stop\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-88199893-2111-4180-8520-81fcba8c00eb-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 3018,\n",
            "            \"output_tokens\": 328,\n",
            "            \"total_tokens\": 3346\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages.human import HumanMessage\n",
        "\n",
        "\n",
        "inputs: dict[str, list[HumanMessage]] = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(json.dumps(\n",
        "            values[\"messages\"],\n",
        "            indent=4,\n",
        "            default=lambda x: x.__dict__,\n",
        "            ensure_ascii=False\n",
        "        ))\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hereâ€™s a breakdown of the steps the agent took to arrive at the correct answer based on the provided trace:\n",
        "\n",
        "1. **Initial Query Handling**:\n",
        "   - The agent received the query related to \"QLoRA\" and decided to perform two actions:\n",
        "     - Search for relevant papers using the `arxiv` tool.\n",
        "     - Search for additional information about the \"QLoRA paper authors\" using the `duckduckgo_search` tool.\n",
        "\n",
        "2. **Tool Execution**:\n",
        "   - The agent executed the `arxiv` tool with the query \"QLoRA.\" This returned detailed information about multiple papers, including their titles, authors, and summaries, which provided crucial details about the QLoRA approach and its authors.\n",
        "   - Simultaneously, the agent executed the `duckduckgo_search` tool with the query \"QLoRA paper authors.\" This search provided additional context and potential author-related information from various sources.\n",
        "\n",
        "3. **Additional Author Information Search**:\n",
        "   - The agent then took the results from the first two searches and initiated another round of searches. This time, the focus was on the latest tweets from the authors of the QLoRA paper, using the `duckduckgo_search` tool:\n",
        "     - Searched for Tim Dettmers' latest tweet.\n",
        "     - Searched for Artidoro Pagnoni's latest tweet.\n",
        "     - Searched for Ari Holtzman's latest tweet.\n",
        "     - Searched for Luke Zettlemoyer's latest tweet.\n",
        "\n",
        "4. **Collection of Author Tweets**:\n",
        "   - The agent successfully retrieved recent tweets or related information about the authors from the search results. This included tweets discussing topics relevant to their research and professional activities.\n",
        "\n",
        "5. **Final Compilation and Summary**:\n",
        "   - The agent compiled the information obtained from all the searches:\n",
        "     - Summarized the QLoRA paper, including the title, authors, and key points from the summary.\n",
        "     - Provided details from the latest tweets of each author, giving additional context and recent activities or opinions related to the field.\n",
        "   - The final response was a comprehensive summary, including both the technical details of the QLoRA paper and recent updates from the authors, ensuring a well-rounded and informed answer.\n",
        "\n",
        "This sequence of actions shows how the agent iteratively gathered information using various tools, cross-referenced data, and compiled the final answer by combining technical details with the latest relevant information from the authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [],
      "source": [
        "response=agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Hereâ€™s a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model needs to produce responses based on a large and diverse set of information, such as in question-answering systems, chatbots, and other applications requiring detailed and accurate information retrieval.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the main innovation introduced in the QLoRA paper?\",\n",
        "    \"How does QLoRA compare to full fine-tuning in terms of memory usage?\",\n",
        "    \"What is the significance of the 4-bit NormalFloat (NF4) data type in QLoRA?\",\n",
        "    \"Who are the authors of the QLoRA paper?\",\n",
        "    \"What is the relationship between QLoRA and PEFT (Parameter-Efficient Fine-Tuning)?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"4-bit quantization\", \"LoRA\", \"memory efficiency\"]},\n",
        "    {\"must_mention\": [\"95% less memory\", \"comparable performance\"]},\n",
        "    {\"must_mention\": [\"NF4\", \"information retention\", \"4-bit quantization\"]},\n",
        "    {\"must_mention\": [\"Tim Dettmers\", \"Artidoro Pagnoni\", \"Ari Holtzman\", \"Luke Zettlemoyer\"]},\n",
        "    {\"must_mention\": [\"PEFT technique\", \"LoRA adaptation\", \"quantization\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created new dataset: 'Langgraph Intro - Evaluation Dataset'\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Langgraph Intro - Evaluation Dataset\"\n",
        "existing_datasets = client.list_datasets()\n",
        "existing_dataset = next((ds for ds in existing_datasets if ds.name == dataset_name), None)\n",
        "if existing_dataset:\n",
        "    dataset = existing_dataset\n",
        "    print(f\"Dataset '{dataset_name}' already exists. Using existing dataset.\")\n",
        "else:\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name, description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        "    )\n",
        "    print(f\"Created new dataset: '{dataset_name}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 5 new examples to the dataset.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from langsmith.schemas import Example\n",
        "\n",
        "\n",
        "def create_new_examples(client, dataset_id, questions, answers):\n",
        "    existing_examples:Example = client.list_examples(dataset_id=dataset_id)\n",
        "    existing_questions = set(example.inputs[\"question\"] for example in existing_examples)\n",
        "    \n",
        "    new_inputs = []\n",
        "    new_outputs = []\n",
        "    \n",
        "    for question, answer in zip(questions, answers):\n",
        "        if question not in existing_questions:\n",
        "            new_inputs.append({\"question\": question})\n",
        "            new_outputs.append(answer)\n",
        "    \n",
        "    if new_inputs:\n",
        "        client.create_examples(\n",
        "            inputs=new_inputs,\n",
        "            outputs=new_outputs,\n",
        "            dataset_id=dataset_id,\n",
        "        )\n",
        "        print(f\"Added {len(new_inputs)} new examples to the dataset.\")\n",
        "    else:\n",
        "        print(\"No new examples to add. All questions already exist in the dataset.\")\n",
        "\n",
        "# Usage\n",
        "create_new_examples(client, dataset.id, questions, answers)\n",
        "# NOTE: the following is commented out as it does not skip when questions\n",
        "# already exist in the dataset\n",
        "# client.create_examples(\n",
        "#     inputs=[{\"question\" : q} for q in questions],\n",
        "#     outputs=answers,\n",
        "#     dataset_id=dataset.id,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ”Answer #3:\n",
        "\n",
        "\n",
        "The correct answers are associated with the questions through the `answers` list, which corresponds to the `questions` list. Each answer in the `answers` list is a dictionary with a \"must_mention\" key, containing a list of phrases or keywords that should be present in a correct response to the corresponding question.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "questions[0] = \"What is the main innovation introduced in the QLoRA paper?\"\n",
        "answers[0] = {\"must_mention\": [\"4-bit quantization\", \"LoRA\", \"memory efficiency\"]}\n",
        "```\n",
        "\n",
        "This approach is somewhat simplistic and could be problematic for a few reasons:\n",
        "1. It doesn't account for synonyms or paraphrasing.\n",
        "2. It might miss correct answers that use different terminology.\n",
        "3. It doesn't consider the context or completeness of the answer.\n",
        "\n",
        "However, it provides a basic way to automatically check if key concepts are present in the response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ”Answer #4:\n",
        "\n",
        "1. Use semantic similarity instead of exact matching, allowing for paraphrasing and synonyms.\n",
        "2. Implement a scoring system based on how many key points are mentioned, rather than a binary pass/fail.\n",
        "3. Use more sophisticated NLP techniques like named entity recognition or topic modeling.\n",
        "4. Incorporate a human-in-the-loop system for edge cases or ambiguous responses.\n",
        "5. Use a pre-trained language model to evaluate answer quality more holistically.\n",
        "6. Include negative examples or common misconceptions to check if they're absent from the response.\n",
        "7. Implement a weighted scoring system where some concepts are more important than others.\n",
        "8. Use regular expressions to allow for variations in phrasing or formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'Langgraph Intro - Evaluation - 876b09af' at:\n",
            "https://smith.langchain.com/o/33fd52fe-2e6b-54d3-84ea-a791c2c90840/datasets/768861e9-d0b8-44e5-b64f-88b833a23b1d/compare?selectedSessions=2df1751c-741e-4782-9275-697b90dd3dd7\n",
            "\n",
            "View all tests for Dataset Langgraph Intro - Evaluation Dataset at:\n",
            "https://smith.langchain.com/o/33fd52fe-2e6b-54d3-84ea-a791c2c90840/datasets/768861e9-d0b8-44e5-b64f-88b833a23b1d\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'Langgraph Intro - Evaluation - 876b09af',\n",
              " 'results': {'67bf1436-415b-4699-a319-167f2594e05c': {'input': {'question': 'What is the main innovation introduced in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('72209a60-c341-46da-874b-499ac4a6f529'), target_run_id=None)],\n",
              "   'execution_time': 4.392089,\n",
              "   'run_id': 'eb4b5eea-1e9b-4dc3-a3a5-9f7874b4b2bf',\n",
              "   'output': 'The main innovation introduced in the QLoRA paper is the development of a novel method called IR-QLoRA (Information Retention Quantized Low-Rank Adaptation). This method aims to enhance the accuracy of quantized large language models (LLMs) that have been fine-tuned using Low-Rank Adaptation (LoRA). The key innovations in IR-QLoRA are:\\n\\n1. **Statistics-based Information Calibration Quantization**: This technique ensures that the quantized parameters of the LLM retain the original information accurately.\\n2. **Finetuning-based Information Elastic Connection**: This allows LoRA to utilize elastic representation transformation with diverse information.\\n\\nThese innovations enable IR-QLoRA to significantly improve the accuracy of LLMs under low bit-widths (2-4 bits) with minimal additional time consumption. The method is also versatile and compatible with various quantization frameworks, providing general accuracy gains across different LLM families.',\n",
              "   'reference': {'must_mention': ['4-bit quantization',\n",
              "     'LoRA',\n",
              "     'memory efficiency']}},\n",
              "  '403b6abd-dbb8-40a3-a24c-d7804b74e846': {'input': {'question': 'How does QLoRA compare to full fine-tuning in terms of memory usage?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5cf8fe88-5827-456d-91e0-826b22407de8'), target_run_id=None)],\n",
              "   'execution_time': 5.361223,\n",
              "   'run_id': 'a1a3bdcd-c8b5-4981-b92a-a88de6b50f46',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) significantly reduces memory usage compared to full fine-tuning, enabling the fine-tuning of large language models (LLMs) on hardware with limited memory. Here are the key points regarding memory usage:\\n\\n1. **Memory Efficiency**: QLoRA reduces memory usage enough to fine-tune a 65 billion parameter model on a single 48GB GPU. This is achieved by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\\n\\n2. **Innovations for Memory Saving**:\\n   - **4-bit NormalFloat (NF4)**: A new data type that is optimal for normally distributed weights.\\n   - **Double Quantization**: Reduces the average memory footprint by quantizing the quantization constants.\\n   - **Paged Optimizers**: Manage memory spikes during training.\\n\\n3. **Comparison to Full Fine-Tuning**: Full fine-tuning typically requires significantly more memory because it involves updating all model parameters. In contrast, QLoRA updates only a small subset of parameters (the low-rank adapters) while keeping the rest of the model in a quantized, frozen state.\\n\\n4. **Performance**: Despite the reduced memory usage, QLoRA preserves the performance of full 16-bit fine-tuning tasks. For example, the Guanaco model family, fine-tuned using QLoRA, outperforms previous openly released models on the Vicuna benchmark, achieving 99.3% of the performance level of ChatGPT.\\n\\nIn summary, QLoRA offers a substantial reduction in memory usage compared to full fine-tuning, making it feasible to fine-tune very large models on hardware with limited memory without sacrificing performance.',\n",
              "   'reference': {'must_mention': ['95% less memory',\n",
              "     'comparable performance']}},\n",
              "  '96e49439-f681-4c8a-84a4-928622d3aed4': {'input': {'question': 'What is the significance of the 4-bit NormalFloat (NF4) data type in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2633c39b-14aa-4a33-b8c5-67981c9c76f4'), target_run_id=None)],\n",
              "   'execution_time': 6.575031,\n",
              "   'run_id': '74b9c23a-6300-43bd-b3fa-6de34b812b68',\n",
              "   'output': 'The 4-bit NormalFloat (NF4) data type plays a significant role in QLoRA (Quantized Low Rank Adaptation) by enabling efficient fine-tuning of large language models while significantly reducing memory usage. Here are the key points about the significance of NF4 in QLoRA:\\n\\n1. **Memory Efficiency**: NF4 is designed to be information-theoretically optimal for normally distributed weights, which are common in pretrained language models. This optimization allows for a substantial reduction in memory usage without sacrificing performance. Specifically, it enables the fine-tuning of a 65 billion parameter model on a single 48GB GPU.\\n\\n2. **Performance Preservation**: Despite the reduction in memory usage, NF4 maintains the performance levels of full 16-bit fine-tuning tasks. This is crucial for ensuring that the fine-tuned models perform well on their intended tasks.\\n\\n3. **Double Quantization**: NF4 is part of a broader strategy in QLoRA that includes double quantization, which further reduces the average memory footprint by quantizing the quantization constants themselves.\\n\\n4. **Paged Optimizers**: Alongside NF4, QLoRA introduces paged optimizers to manage memory spikes, ensuring that the memory usage remains efficient throughout the fine-tuning process.\\n\\n5. **Broad Applicability**: The use of NF4 in QLoRA has been demonstrated across various model types (e.g., LLaMA, T5) and scales (e.g., 33B and 65B parameter models), showing its versatility and effectiveness in different contexts.\\n\\n6. **State-of-the-Art Results**: Fine-tuning with QLoRA and NF4 on small, high-quality datasets has led to state-of-the-art results, even when using smaller models compared to previous state-of-the-art approaches.\\n\\nIn summary, the 4-bit NormalFloat (NF4) data type is a critical innovation in QLoRA that enables efficient and effective fine-tuning of large language models by optimizing memory usage while preserving performance.',\n",
              "   'reference': {'must_mention': ['NF4',\n",
              "     'information retention',\n",
              "     '4-bit quantization']}},\n",
              "  '7d29720e-777a-47a7-a876-f6eff2bbd936': {'input': {'question': 'Who are the authors of the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7cb28b3a-cbf2-4a69-9b18-7befe9553d66'), target_run_id=None)],\n",
              "   'execution_time': 2.311453,\n",
              "   'run_id': '83bd9bcb-d9ea-4a23-a572-1f6b90c881c7',\n",
              "   'output': 'The authors of the QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" are:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer',\n",
              "   'reference': {'must_mention': ['Tim Dettmers',\n",
              "     'Artidoro Pagnoni',\n",
              "     'Ari Holtzman',\n",
              "     'Luke Zettlemoyer']}},\n",
              "  '9f48b75c-1517-41c2-9470-3fe54ea92107': {'input': {'question': 'What is the relationship between QLoRA and PEFT (Parameter-Efficient Fine-Tuning)?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8b3e1de5-50ce-4c8e-929d-f0fbc036c946'), target_run_id=None)],\n",
              "   'execution_time': 10.235748,\n",
              "   'run_id': '62d299a0-babb-4fa6-a385-b868573dfe8c',\n",
              "   'output': \"### Relationship between QLoRA and PEFT (Parameter-Efficient Fine-Tuning)\\n\\n**QLoRA (Quantized Low-Rank Adaptation)**:\\n- **Definition**: QLoRA is an efficient fine-tuning approach designed to reduce memory usage while maintaining high performance. It allows fine-tuning of large language models (LLMs) on resource-constrained hardware by using a combination of 4-bit quantization and Low-Rank Adapters (LoRA).\\n- **Key Innovations**:\\n  - **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\\n  - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\\n  - **Paged Optimizers**: Manages memory spikes during training.\\n- **Performance**: QLoRA has been shown to achieve state-of-the-art results with significantly reduced computational resources. For example, it can fine-tune a 65B parameter model on a single 48GB GPU.\\n\\n**PEFT (Parameter-Efficient Fine-Tuning)**:\\n- **Definition**: PEFT techniques aim to fine-tune large pre-trained models by adding a small number of trainable parameters while keeping the majority of the pre-trained parameters fixed. This approach addresses issues of overfitting and high computational costs associated with fully fine-tuning large models.\\n- **Common Methods**:\\n  - **Low-Rank Adapters (LoRA)**: Adds low-rank matrices to the model's layers, which are the only parameters updated during fine-tuning.\\n  - **Other Techniques**: Include methods like prompt tuning, adapter tuning, and more, which also focus on updating a small subset of parameters.\\n\\n**Relationship**:\\n- **Integration**: QLoRA is a specific implementation of PEFT. It combines the principles of PEFT with quantization techniques to further reduce memory usage and computational requirements.\\n- **Efficiency**: Both QLoRA and PEFT aim to make the fine-tuning of large models more efficient. QLoRA extends the concept of PEFT by incorporating quantization, which allows for even more significant reductions in resource usage.\\n- **Performance**: QLoRA demonstrates that PEFT methods can achieve high performance even with substantial reductions in the number of trainable parameters and memory usage.\\n\\nIn summary, QLoRA can be seen as an advanced form of PEFT that leverages quantization to enhance the efficiency and feasibility of fine-tuning large language models on limited hardware.\",\n",
              "   'reference': {'must_mention': ['PEFT technique',\n",
              "     'LoRA adaptation',\n",
              "     'quantization']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"Langgraph Intro - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### Initializing the Graph and Adding Nodes\n",
        "\n",
        "This code initializes our LangGraph and adds two key nodes:\n",
        "\n",
        "1. We create a new `StateGraph` called `graph_with_helpfulness_check`, using our previously defined `AgentState` as the state type.\n",
        "\n",
        "2. We add two nodes to the graph:\n",
        "   - An \"agent\" node that calls our language model (using the `call_model` function).\n",
        "   - An \"action\" node that executes tool actions (using the `tool_node` function).\n",
        "\n",
        "These nodes form the core of our agent's decision-making and action-execution process. The \"agent\" node will be responsible for deciding what to do next, while the \"action\" node will carry out the chosen actions.\n",
        "\n",
        "This structure allows for a flexible, graph-based approach to agent design, as described in the Pinecone article on LangGraph and research agents [1]. By using a graph structure, we can more easily customize the agent's behavior and add additional nodes or edges as needed.\n",
        "\n",
        "[1]: https://www.pinecone.io/learn/langgraph-research-agent/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", action = lambda state: call_model(state, runnable))\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### Setting the Entry Point\n",
        "\n",
        "\n",
        "This line of code sets the entry point of our LangGraph to the \"agent\" node. Here's what this means:\n",
        "\n",
        "1. The entry point is the first node that will be executed when the graph is run.\n",
        "2. By setting it to \"agent\", we're indicating that our graph execution should always start with the agent making a decision or taking an action.\n",
        "3. This is a crucial step in defining the flow of our agent-based system, as it determines where the execution begins each time the graph is invoked.\n",
        "\n",
        "Setting the entry point to \"agent\" aligns with the typical structure of an agent-based system, where the agent is the primary decision-maker that initiates actions or responses. This setup allows the agent to assess the current state and decide on the next steps, whether it's to use a tool, respond to the user, or take any other action defined in our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### Decision-making node\n",
        "\n",
        "\n",
        "The `tool_call_or_helpful` function is a crucial part of the LangGraph implementation, serving as a decision-making node in the graph. Here's a breakdown of what's happening in this function:\n",
        "\n",
        "1. State Examination:\n",
        "   - It first checks the last message in the state for any tool calls.\n",
        "   - If tool calls are present, it returns \"action\", indicating that a tool should be used.\n",
        "\n",
        "2. Message Limit Check:\n",
        "   - If the number of messages in the state exceeds 10, it returns \"END\", preventing infinite loops.\n",
        "\n",
        "3. Helpfulness Evaluation:\n",
        "   - If no tool calls are needed and the message limit isn't reached, it proceeds to evaluate the helpfulness of the response.\n",
        "   - It extracts the initial query and the final response from the state.\n",
        "\n",
        "4. Prompt Creation:\n",
        "   - A prompt template is defined to assess the helpfulness of the response.\n",
        "\n",
        "5. Model Setup:\n",
        "   - It initializes a ChatOpenAI model (GPT-4) for the helpfulness check.\n",
        "\n",
        "6. Helpfulness Chain:\n",
        "   - A chain is created using the prompt template, the GPT-4 model, and a string output parser.\n",
        "\n",
        "7. Helpfulness Assessment:\n",
        "   - The chain is invoked with the initial query and final response.\n",
        "   - The model determines if the response is helpful, returning 'Y' for yes or 'N' for no.\n",
        "\n",
        "8. Final Decision:\n",
        "   - If the response is deemed helpful ('Y'), it returns \"end\", concluding the process.\n",
        "   - If not helpful, it returns \"continue\", allowing the graph to cycle again for improvement.\n",
        "\n",
        "This function embodies key concepts of LangGraph, such as state management,\n",
        "conditional logic, and the use of language models for decision-making within the\n",
        "graph structure. \n",
        "\n",
        "It demonstrates how LangGraph can create more sophisticated, self-improving AI systems by incorporating feedback loops and decision points based on the quality of responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import END\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### Adding Conditional Logic\n",
        "\n",
        "This code snippet adds conditional edges to the `graph_with_helpfulness_check` StateGraph. Here's a breakdown of what it does:\n",
        "\n",
        "1. It starts from the \"agent\" node.\n",
        "\n",
        "2. It uses the `tool_call_or_helpful` function to determine the next step in the graph. This function likely evaluates the agent's response to decide whether to:\n",
        "   - Continue with the agent (\"continue\")\n",
        "   - Perform an action (\"action\")\n",
        "   - End the conversation (\"end\")\n",
        "\n",
        "3. The conditional edges are defined with a dictionary mapping the possible outcomes to their corresponding nodes:\n",
        "   - If the result is \"continue\", it loops back to the \"agent\" node.\n",
        "   - If the result is \"action\", it moves to the \"action\" node (likely to execute a tool).\n",
        "   - If the result is \"end\", it terminates the graph execution using the special END node.\n",
        "\n",
        "This structure allows for a flexible conversation flow, where the agent can decide to continue thinking, use a tool, or conclude the interaction based on the current state of the conversation. It's a key component in implementing the cyclic behavior characteristic of LangGraph, as described in the LangGraph documentation [1].\n",
        "\n",
        "[1]: https://langchain-ai.github.io/langgraph/how-tos/tool-calling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### Adding an Edge from Action to Agent\n",
        "\n",
        "This line of code adds a directed edge in our graph from the \"action\" node to the \"agent\" node. Here's what this means:\n",
        "\n",
        "1. After an action is performed (e.g., using a tool), the graph will always return to the agent node.\n",
        "2. This creates a cycle in our graph, allowing the agent to potentially use multiple tools in sequence if needed.\n",
        "3. It ensures that after every action, the agent has a chance to evaluate the new state and decide on the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### Compiling the Graph\n",
        "\n",
        "This line of code compiles the `graph_with_helpfulness_check` into an executable\n",
        "agent\n",
        "\n",
        "Compiling the graph is a crucial step in the LangGraph workflow. Here's what this does:\n",
        "\n",
        "1. Validation: It performs basic checks on the structure of the graph, ensuring there are no orphaned nodes or other structural issues.\n",
        "\n",
        "2. Optimization: It may perform some internal optimizations to improve the execution efficiency of the graph.\n",
        "\n",
        "3. Finalization: It finalizes the graph structure, making it ready for execution.\n",
        "\n",
        "4. Runnable Creation: It turns the graph into a LangChain Runnable object, which provides methods like `.invoke()`, `.stream()`, and `.batch()` for executing the graph with inputs.\n",
        "\n",
        "5. Runtime Arguments: While not shown in this specific example, the `compile()` method is where you can specify runtime arguments like checkpointers and breakpoints if needed.\n",
        "\n",
        "Compiling is a necessary step before you can use your graph. It transforms the graph definition into an executable form, preparing it for actual use in your application.\n",
        "\n",
        "This step aligns with the LangGraph documentation, which states: \n",
        "\n",
        "> You **MUST** compile your graph before you can use it [1].\n",
        "\n",
        "[1]: https://langchain-ai.github.io/langgraph/concepts/low_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "### Testing the Agent with Helpfulness Check\n",
        "\n",
        "This code sets up a test for the agent we've created that includes a helpfulness check. Here's a breakdown of what it does:\n",
        "\n",
        "\n",
        "1. It imports the `HumanMessage` class from LangChain, which represents a message from a human user.\n",
        "\n",
        "2. It creates an input dictionary with a single message asking about LoRA, Tim Dettmers, and Attention in machine learning.\n",
        "\n",
        "3. It uses the `astream` method of our `agent_with_helpfulness_check` to asynchronously stream the agent's response. The `stream_mode=\"updates\"` parameter tells it to stream updates to the state after each step of the graph.\n",
        "\n",
        "4. For each chunk of the streamed response:\n",
        "   - It prints the name of the node that produced the update.\n",
        "   - It prints the messages in the current state, formatted as JSON for readability.\n",
        "\n",
        "This setup allows us to see how the agent processes the query step by step, including any tool usage and the final response. It's particularly useful for debugging and understanding the agent's decision-making process.\n",
        "\n",
        "The use of `astream` and the streaming setup aligns with LangGraph's support for streaming, as described in the LangGraph documentation [1]. This approach allows for real-time observation of the agent's thought process and actions.\n",
        "\n",
        "[1]: https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"tool_calls\": [\n",
            "                {\n",
            "                    \"id\": \"call_F2h2FWdsRoMYhwLjJEbKhvlt\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"LoRA machine learning\\\"}\",\n",
            "                        \"name\": \"arxiv\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_3G2WsdcvLZ3BLS1KsSWeW4j9\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Tim Dettmers\\\"}\",\n",
            "                        \"name\": \"duckduckgo_search\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"call_ivKuAVxeaggFuHPZTUzhLfZY\",\n",
            "                    \"function\": {\n",
            "                        \"arguments\": \"{\\\"query\\\": \\\"Attention mechanism machine learning\\\"}\",\n",
            "                        \"name\": \"arxiv\"\n",
            "                    },\n",
            "                    \"type\": \"function\"\n",
            "                }\n",
            "            ],\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 70,\n",
            "                \"prompt_tokens\": 171,\n",
            "                \"total_tokens\": 241\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"tool_calls\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-d35ae030-95bc-45cf-b2bd-7c2d51f42322-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [\n",
            "            {\n",
            "                \"name\": \"arxiv\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"LoRA machine learning\"\n",
            "                },\n",
            "                \"id\": \"call_F2h2FWdsRoMYhwLjJEbKhvlt\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"duckduckgo_search\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Tim Dettmers\"\n",
            "                },\n",
            "                \"id\": \"call_3G2WsdcvLZ3BLS1KsSWeW4j9\",\n",
            "                \"type\": \"tool_call\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"arxiv\",\n",
            "                \"args\": {\n",
            "                    \"query\": \"Attention mechanism machine learning\"\n",
            "                },\n",
            "                \"id\": \"call_ivKuAVxeaggFuHPZTUzhLfZY\",\n",
            "                \"type\": \"tool_call\"\n",
            "            }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 171,\n",
            "            \"output_tokens\": 70,\n",
            "            \"total_tokens\": 241\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"Published: 2024-06-17\\nTitle: Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead\\nAuthors: Rickard BrÃ¼el-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon\\nSummary: Fine-tuning large language models (LLMs) with low-rank adapters (LoRAs) has\\nbecome common practice, often yielding numerous copies of the same LLM\\ndiffering only in their LoRA updates. This paradigm presents challenges for\\nsystems that serve real-time responses to queries that each involve a different\\nLoRA. Prior works optimize the design of such systems but still require\\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\\nefficacy of compression when serving LoRA adapters. We consider compressing\\nadapters individually via SVD and propose a method for joint compression of\\nLoRAs into a shared basis paired with LoRA-specific scaling matrices. Our\\nexperiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve\\nperformance while offering major throughput gains in realistic serving\\nscenarios with over a thousand LoRAs, maintaining 75% of the throughput of\\nserving a single LoRA.\\n\\nPublished: 2024-04-07\\nTitle: A Note on LoRA\\nAuthors: Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, Weizhu Chen\\nSummary: LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently\\nadapting Large Language Models (LLMs) with remarkable simplicity and efficacy.\\nThis note extends the original LoRA paper by offering new perspectives that\\nwere not initially discussed and presents a series of insights for deploying\\nLoRA at scale. Without introducing new experiments, we aim to improve the\\nunderstanding and application of LoRA.\\n\\nPublished: 2024-06-18\\nTitle: LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation\\nAuthors: Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao, Muyun Yang\\nSummary: Low-Rank Adaptation (LoRA) is currently the most commonly used\\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\\nparameters for each layer to fine-tune the pre-trained model under limited\\ncomputing resources. However, it still faces resource consumption challenges\\nduring training when scaling up to larger models. Most previous studies have\\ntackled this issue by using pruning techniques, which involve removing LoRA\\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\\nparameter features to evaluate their importance, such as parameter count, size,\\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\\nstate), directly impacts the final results. Preliminary experiments indicate\\nthat a fraction of LoRA elements possesses significantly high output values,\\nsubstantially influencing the layer output. Motivated by the observation, we\\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\\non the LoRA output. Then we retain LoRA for important layers and the other\\nlayers share the same LoRA. We conduct abundant experiments with models of\\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\\nachieve performance comparable to full fine-tuning and LoRA, while retaining\\n50\\\\% of the LoRA parameters on average.\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"arxiv\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_F2h2FWdsRoMYhwLjJEbKhvlt\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required â€” from over 780GB to less than 48GB â€” to finetune a 65B parameter model. She also spent time at Google Research and Salesforce Research. â€” Tim Dettmers is joining Ai2 as an AI researcher. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.â€¦\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"duckduckgo_search\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_3G2WsdcvLZ3BLS1KsSWeW4j9\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    },\n",
            "    {\n",
            "        \"content\": \"Published: 2022-03-27\\nTitle: A General Survey on Attention Mechanisms in Deep Learning\\nAuthors: Gianni Brauwers, Flavius Frasincar\\nSummary: Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.\\n\\nPublished: 2022-07-04\\nTitle: Attention mechanisms for physiological signal deep learning: which attention should we take?\\nAuthors: Seong-A Park, Hyung-Chul Lee, Chul-Woo Jung, Hyun-Lim Yang\\nSummary: Attention mechanisms are widely used to dramatically improve deep learning\\nmodel performance in various fields. However, their general ability to improve\\nthe performance of physiological signal deep learning model is immature. In\\nthis study, we experimentally analyze four attention mechanisms (e.g.,\\nsqueeze-and-excitation, non-local, convolutional block attention module, and\\nmulti-head self-attention) and three convolutional neural network (CNN)\\narchitectures (e.g., VGG, ResNet, and Inception) for two representative\\nphysiological signal prediction tasks: the classification for predicting\\nhypotension and the regression for predicting cardiac output (CO). We evaluated\\nmultiple combinations for performance and convergence of physiological signal\\ndeep learning model. Accordingly, the CNN models with the spatial attention\\nmechanism showed the best performance in the classification problem, whereas\\nthe channel attention mechanism achieved the lowest error in the regression\\nproblem. Moreover, the performance and convergence of the CNN models with\\nattention mechanisms were better than stand-alone self-attention models in both\\nproblems. Hence, we verified that convolutional operation and attention\\nmechanisms are complementary and provide faster convergence time, despite the\\nstand-alone self-attention models requiring fewer parameters.\\n\\nPublished: 2018-10-17\\nTitle: An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation\\nAuthors: Gongbo Tang, Rico Sennrich, Joakim Nivre\\nSummary: Recent work has shown that the encoder-decoder attention mechanisms in neural\\nmachine translation (NMT) are different from the word alignment in statistical\\nmachine translation. In this paper, we focus on analyzing encoder-decoder\\nattention mechanisms, in the case of word sense disambiguation (WSD) in NMT\\nmodels. We hypothesize that attention mechanisms pay more attention to context\\ntokens when translating ambiguous words. We explore the attention distribution\\npatterns when translating ambiguous nouns. Counter-intuitively, we find that\\nattention mechanisms are likely to distribute more attention to the ambiguous\\nnoun itself rather than context tokens, in comparison to other nouns. We\\nconclude that attention mechanism is not the main mechanism used by NMT models\\nto incorporate contextual information for WSD. The experimental results suggest\\nthat NMT models learn to encode contextual information necessary for WSD in the\\nencoder hidden states. For the attention mechanism in Transformer models, we\\nreveal that the first few layers gradually learn to \\\"align\\\" source and target\\ntokens and the last few layers learn to extract features from the related but\\nunaligned context tokens.\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"type\": \"tool\",\n",
            "        \"name\": \"arxiv\",\n",
            "        \"id\": null,\n",
            "        \"tool_call_id\": \"call_ivKuAVxeaggFuHPZTUzhLfZY\",\n",
            "        \"artifact\": null,\n",
            "        \"status\": \"success\"\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[\n",
            "    {\n",
            "        \"content\": \"### LoRA (Low-Rank Adaptation)\\nLoRA is a method used in machine learning, particularly for fine-tuning large language models (LLMs). It introduces low-rank adapters to efficiently adapt pre-trained models with limited computational resources. Here are some key points from recent research:\\n\\n1. **Compress then Serve**: This study investigates the efficacy of compressing LoRA adapters to serve thousands of them with little overhead. The method involves compressing adapters individually via Singular Value Decomposition (SVD) and proposes a joint compression method. The experiments show that compressed LoRAs can preserve performance while offering significant throughput gains.\\n\\n2. **A Note on LoRA**: This paper extends the original LoRA concept by providing new perspectives and insights for deploying LoRA at scale. It aims to improve the understanding and application of LoRA without introducing new experiments.\\n\\n3. **LoRA-drop**: This research proposes a method called LoRA-drop, which evaluates the importance of LoRA parameters based on their output. It retains important LoRA parameters and shares the same LoRA for less important layers. The results demonstrate that LoRA-drop can achieve performance comparable to full fine-tuning while retaining only 50% of the LoRA parameters on average.\\n\\n### Tim Dettmers\\nTim Dettmers is a researcher focused on making foundation models, such as ChatGPT, more accessible by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems for memory-efficient, fast, and cost-effective deep learning. He has contributed significantly to the field, including a notable approach called QLoRA, which reduces the GPU memory required for fine-tuning large models.\\n\\n### Attention Mechanism in Machine Learning\\nAttention mechanisms are crucial in various deep learning models, particularly in natural language processing (NLP). They allow models to focus on different parts of the input sequence when making predictions. Here are some insights from recent research:\\n\\n1. **General Survey on Attention Mechanisms**: This survey provides an overview of the most important attention mechanisms in deep learning. It introduces a framework for understanding different attention models, including a comprehensive taxonomy and evaluation measures.\\n\\n2. **Attention Mechanisms for Physiological Signal Deep Learning**: This study analyzes different attention mechanisms and their performance in physiological signal prediction tasks. It finds that spatial attention mechanisms perform best in classification tasks, while channel attention mechanisms achieve the lowest error in regression tasks. The study also shows that combining convolutional operations with attention mechanisms provides faster convergence.\\n\\n3. **Analysis of Attention Mechanisms in Neural Machine Translation**: This paper focuses on the role of attention mechanisms in word sense disambiguation (WSD) in neural machine translation (NMT). It finds that attention mechanisms tend to focus more on ambiguous nouns rather than context tokens, suggesting that NMT models use encoder hidden states to encode contextual information necessary for WSD.\\n\\nThese insights highlight the versatility and importance of attention mechanisms in improving the performance and efficiency of deep learning models across various domains.\",\n",
            "        \"additional_kwargs\": {\n",
            "            \"refusal\": null\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 602,\n",
            "                \"prompt_tokens\": 2015,\n",
            "                \"total_tokens\": 2617\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o-2024-05-13\",\n",
            "            \"system_fingerprint\": \"fp_157b3831f5\",\n",
            "            \"finish_reason\": \"stop\",\n",
            "            \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ai\",\n",
            "        \"name\": null,\n",
            "        \"id\": \"run-0c665508-a060-4083-afa4-9ce49d78552e-0\",\n",
            "        \"example\": false,\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "            \"input_tokens\": 2015,\n",
            "            \"output_tokens\": 602,\n",
            "            \"total_tokens\": 2617\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages.human import HumanMessage\n",
        "\n",
        "inputs: dict[str, list[HumanMessage]] = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(json.dumps(\n",
        "            values[\"messages\"],\n",
        "            indent=4,\n",
        "            default=lambda x: x.__dict__,\n",
        "            ensure_ascii=False\n",
        "        ))\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### What is Prompt Engineering?\n",
            "\n",
            "Prompt engineering is the process of creating and refining input prompts to guide generative AI models, especially those using natural language processing (NLP). This involves designing inputs that help AI systems understand tasks and generate accurate responses. The goal is to optimize the interaction between humans and AI by carefully crafting input prompts to help the language model understand the context and produce the desired output.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "- **Refinement**: Continuously improving the prompts to get better results.\n",
            "- **Structure and Language**: Using the proper structure and language to ensure the AI understands the task.\n",
            "- **Optimization**: Enhancing the output of large language models (LLMs) like OpenAI's GPT-4.\n",
            "- **Application**: Used in various generative AI models such as ChatGPT, DALL-E, and others.\n",
            "\n",
            "### History of Prompt Engineering\n",
            "\n",
            "Prompt engineering has evolved significantly over the years, particularly in the field of natural language processing (NLP). Here are some key milestones:\n",
            "\n",
            "1. **Early Language Models and Information Retrieval Systems**: The roots of prompt engineering can be traced back to the early days of language models and information retrieval systems.\n",
            "2. **Introduction of Attention Mechanisms (2015)**: The introduction of attention mechanisms revolutionized language understanding and significantly impacted prompt engineering.\n",
            "3. **Modern AI-Powered Solutions**: The field has transformed with the advent of modern AI-powered solutions, moving beyond simple instructions to meticulously crafted prompts that elicit specific responses.\n",
            "4. **Ethical Considerations**: The evolution of prompt engineering also includes a focus on ethical considerations, ensuring responsible human-AI collaboration and aligning language models closely with human intentions.\n",
            "\n",
            "Prompt engineering has become a crucial aspect of machine learning and artificial intelligence, shaping the future of AI applications and fostering responsible and effective human-AI interactions.\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by researchers at Facebook AI in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both retrieval and generation to handle tasks that require access to a large amount of external knowledge, such as open-domain question answering, dialogue systems, and more.\n",
            "\n",
            "Would you like more detailed information about the RAG model or its applications?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This process leverages the knowledge the model has already acquired during its initial training on a large, general dataset, and refines it to perform better on a more specific task or domain. Fine-tuning is particularly useful in transfer learning, where the goal is to transfer knowledge from one domain to another.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: Train a model on a large, general dataset.\n",
            "2. **Fine-tuning**: Continue training the pre-trained model on a smaller, task-specific dataset.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Reduces the amount of data and computational resources needed.\n",
            "- **Performance**: Often leads to better performance on the specific task compared to training a model from scratch.\n",
            "- **Adaptability**: Allows models to be adapted to new tasks quickly.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the availability of large pre-trained models. One of the key moments in its rise to prominence was the development and success of models like BERT (Bidirectional Encoder Representations from Transformers) by Google in 2018, which demonstrated the power of fine-tuning in natural language processing tasks.\n",
            "\n",
            "To get more precise information on the historical context and key milestones, I can look up relevant articles and sources. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "### What are LLM-based Agents?\n",
            "\n",
            "LLM-based agents are autonomous systems that leverage Large Language Models (LLMs) to perform complex tasks. These agents can interact with their environment, perceive data, and take actions based on the information they process. They are designed to handle a variety of tasks, often in specialized domains such as software engineering, code generation, and vulnerability detection.\n",
            "\n",
            "### Key Features:\n",
            "1. **Autonomy**: LLM-based agents can operate independently, making decisions and taking actions without human intervention.\n",
            "2. **Perception and Action**: These agents can sense their environment and act upon it, often using tools to achieve their goals.\n",
            "3. **Multi-Agent Systems**: LLM-based agents can work in concert with other agents to achieve common objectives, enhancing their task-solving capabilities.\n",
            "\n",
            "### Historical Context:\n",
            "The concept of LLM-based agents has been a focal point of research in both academia and industry. The rise of powerful LLMs has spurred a new trend in building these autonomous agents, particularly for solving complex tasks in multi-agent systems.\n",
            "\n",
            "### Notable Research:\n",
            "1. **EvoAgent (2024)**: This research introduces a method to automatically extend expert agents to multi-agent systems using evolutionary algorithms, thereby improving the effectiveness of LLM-based agents in solving tasks.\n",
            "2. **Cooperative Heterogeneous Deep Reinforcement Learning (2020)**: This framework integrates the advantages of heterogeneous agents, classifying them into global and local agents to improve task performance.\n",
            "3. **Improving Robotic Virtual Agent's Errors (2023)**: This study focuses on human-agent interaction, particularly how agent reactions and human preferences affect empathy and acceptance of the agent's mistakes.\n",
            "\n",
            "### When Did LLM-based Agents Break onto the Scene?\n",
            "The concept of LLM-based agents has been evolving over the past few years, with significant research contributions emerging around 2020. The field has seen rapid advancements, particularly with the development of more sophisticated LLMs and multi-agent systems.\n",
            "\n",
            "### Conclusion\n",
            "LLM-based agents represent a significant advancement in the field of artificial intelligence, offering new ways to automate complex tasks and interact with environments. Their development has been driven by the increasing capabilities of LLMs and the need for more autonomous, efficient systems.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- https://github.com/langchain-ai/langgraph/blob/main/libs/langgraph/langgraph/prebuilt/chat_agent_executor.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
